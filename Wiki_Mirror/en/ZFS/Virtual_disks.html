<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>ZFS/Virtual disks - ArchWiki</title>
<link rel="stylesheet" href="../../ArchWikiOffline.css">
<meta name="ResourceLoaderDynamicStyles" content="">
<meta name="generator" content="MediaWiki 1.35.0">
<meta name="referrer" content="no-referrer-when-downgrade">
<meta name="robots" content="noindex,follow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/opensearch_desc.php" title="ArchWiki (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://wiki.archlinux.org/api.php?action=rsd">
<link rel="license" href="http://www.gnu.org/copyleft/fdl.html">
<link rel="alternate" type="application/atom+xml" title="ArchWiki Atom feed" href="/index.php?title=Special:RecentChanges&amp;feed=atom">
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-ZFS_Virtual_disks rootpage-ZFS skin-vector action-view skin-vector-legacy">
<div id="content" class="mw-body" role="main" style="margin: 0">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"></div>
	<div class="mw-indicators mw-body-content">
	</div>
	<h1 id="firstHeading" class="firstHeading" lang="en">ZFS/Virtual disks</h1>
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From ArchWiki</div>
		<div id="contentSub"><span class="subpages">&lt; <a href="/title/ZFS" title="ZFS">ZFS</a></span></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr">
<div class="warningbox">The printable version is no longer supported and may have rendering errors. Please update your browser bookmarks and please use the default browser print function instead.</div>
<div class="mw-parser-output">
<div class="archwiki-template-meta-related-articles-start">
<p>Related articles</p>
<ul>
<li><a href="/title/ZFS" title="ZFS">ZFS</a></li>
<li><a href="/title/Installing_Arch_Linux_on_ZFS" class="mw-redirect" title="Installing Arch Linux on ZFS">Installing Arch Linux on ZFS</a></li>
</ul>
</div>
<p>This article covers some basic tasks and usage of ZFS. It differs from the main article <a href="/title/ZFS" title="ZFS">ZFS</a> somewhat in that the examples herein are demonstrated on a zpool built from virtual disks. So long as users do not place any critical data on the resulting zpool, they are free to experiment without fear of actual data loss.
</p>
<p>The examples in this article are shown with a set of virtual discs known in ZFS terms as VDEVs. Users may create their VDEVs either on an existing physical disk or in tmpfs (RAMdisk) depending on the amount of free memory on the system.
</p>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> Using a file as a VDEV is a great method to play with ZFS but is not viable strategy for storing "real" data.</div>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading">
<input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr">
<h2 id="mw-toc-heading">Contents</h2>
<span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Install_the_ZFS_family_of_packages"><span class="tocnumber">1</span> <span class="toctext">Install the ZFS family of packages</span></a></li>
<li class="toclevel-1 tocsection-2">
<a href="#Creating_and_destroying_Zpools"><span class="tocnumber">2</span> <span class="toctext">Creating and destroying Zpools</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Mirror"><span class="tocnumber">2.1</span> <span class="toctext">Mirror</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#RAIDZ1"><span class="tocnumber">2.2</span> <span class="toctext">RAIDZ1</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#RAIDZ2_and_RAIDZ3"><span class="tocnumber">2.3</span> <span class="toctext">RAIDZ2 and RAIDZ3</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Linear_span"><span class="tocnumber">2.4</span> <span class="toctext">Linear span</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Creating_and_destroying_datasets"><span class="tocnumber">3</span> <span class="toctext">Creating and destroying datasets</span></a></li>
<li class="toclevel-1 tocsection-8">
<a href="#Displaying_and_setting_properties"><span class="tocnumber">4</span> <span class="toctext">Displaying and setting properties</span></a>
<ul>
<li class="toclevel-2 tocsection-9"><a href="#Show_properties"><span class="tocnumber">4.1</span> <span class="toctext">Show properties</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Modify_properties"><span class="tocnumber">4.2</span> <span class="toctext">Modify properties</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Add_content_to_the_Zpool_and_query_compression_performance"><span class="tocnumber">5</span> <span class="toctext">Add content to the Zpool and query compression performance</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Simulate_a_disk_failure_and_rebuild_the_Zpool"><span class="tocnumber">6</span> <span class="toctext">Simulate a disk failure and rebuild the Zpool</span></a></li>
<li class="toclevel-1 tocsection-13">
<a href="#Snapshots_and_recovering_deleted_files"><span class="tocnumber">7</span> <span class="toctext">Snapshots and recovering deleted files</span></a>
<ul>
<li class="toclevel-2 tocsection-14"><a href="#Time_0"><span class="tocnumber">7.1</span> <span class="toctext">Time 0</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Time_+1"><span class="tocnumber">7.2</span> <span class="toctext">Time +1</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Time_+2"><span class="tocnumber">7.3</span> <span class="toctext">Time +2</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Time_+3"><span class="tocnumber">7.4</span> <span class="toctext">Time +3</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Time_+4"><span class="tocnumber">7.5</span> <span class="toctext">Time +4</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Listing_snapshots"><span class="tocnumber">7.6</span> <span class="toctext">Listing snapshots</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Deleting_snapshots"><span class="tocnumber">7.7</span> <span class="toctext">Deleting snapshots</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="#Troubleshooting"><span class="tocnumber">8</span> <span class="toctext">Troubleshooting</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Install_the_ZFS_family_of_packages">Install the ZFS family of packages</span></h2>
<p>Due to differences in licencing, ZFS bins and kernel modules are easily distributed from source, but no-so-easily packaged as pre-compiled sets.  The requisite packages are available in the AUR and in an unofficial repo.  Details are provided on the <a href="/title/ZFS#Installation" title="ZFS">ZFS#Installation</a> article.
</p>
<h2><span class="mw-headline" id="Creating_and_destroying_Zpools">Creating and destroying Zpools</span></h2>
<p>Management of ZFS is pretty simplistic with only two utils needed:
</p>
<ul>
<li><code>/usr/bin/zpool</code></li>
<li><code>/usr/bin/zfs</code></li>
</ul>
<h3><span class="mw-headline" id="Mirror">Mirror</span></h3>
<p>For zpools with just two drives with redundancy, it is recommended to use ZFS in <i>mirror</i> mode which functions like a RAID1 mirroring the data.  Mirroring can also be used as an alternative to Raidz setups with surprising results.  See more on vdev mirroring <a rel="nofollow" class="external text" href="https://jrs-s.net/2015/02/06/zfs-you-should-use-mirror-vdevs-not-raidz/">here</a>.
</p>
<h3><span class="mw-headline" id="RAIDZ1">RAIDZ1</span></h3>
<p>The minimum number of drives for a RAIDZ1 is three.  It's best to follow the "power of two plus parity" recommendation.  This is for storage space efficiency and hitting the "sweet spot" in performance.  For RAIDZ-1, use three (2+1), five (4+1), or nine (8+1) disks. This example will use the most simplistic set of (2+1).
</p>
<p>Create three x 2G files to serve as virtual hardrives:
</p>
<pre>$ for i in {1..3}; do truncate -s 2G /scratch/$i.img; done
</pre>
<p>Assemble the RAIDZ1:
</p>
<pre># zpool create zpool raidz1 /scratch/1.img /scratch/2.img /scratch/3.img
</pre>
<p>Notice that a 3.91G zpool has been created and mounted for us:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;"> NAME   USED  AVAIL  REFER  MOUNTPOINT
 test   139K  3.91G  38.6K  /zpool
</pre>
<p>The status of the device can be queried:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zpool status zpool</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">  pool: zpool
 state: ONLINE
  scan: none requested
config:

	NAME                STATE     READ WRITE CKSUM
	   zpool            ONLINE       0     0     0
	  raidz1-0          ONLINE       0     0     0
	    /scratch/1.img  ONLINE       0     0     0
	    /scratch/2.img  ONLINE       0     0     0
	    /scratch/3.img  ONLINE       0     0     0

errors: No known data errors
</pre>
<p>To destroy a zpool:
</p>
<pre># zpool destroy zpool
</pre>
<h3><span class="mw-headline" id="RAIDZ2_and_RAIDZ3">RAIDZ2 and RAIDZ3</span></h3>
<p>Higher level ZRAIDs can be assembled in a like fashion by adjusting the for statement to create the image files, by specifying "raidz2" or "raidz3" in the creation step, and by appending the additional image files to the creation step.
</p>
<p>Summarizing Toponce's guidance:
</p>
<ul>
<li>RAIDZ2 should use four (2+2), six (4+2), ten (8+2), or eighteen (16+2) disks.</li>
<li>RAIDZ3 should use five (2+3), seven (4+3), eleven (8+3), or nineteen (16+3) disks.</li>
</ul>
<h3><span class="mw-headline" id="Linear_span">Linear span</span></h3>
<p>This setup is for a JBOD, good for 3 or less drives normally, where space is still a concern and you are not ready to move to full features of ZFS yet because of it.  RaidZ will be your better bet once you achieve enough space to satisfy, since this setup is NOT taking advantage of the full features of ZFS, but has its roots safely set in a beginning array that will suffice for years until you build up your hard drive collection.  
</p>
<p>Assemble the Linear Span:
</p>
<pre># zpool create zpool san /dev/sdd /dev/sde /dev/sdf
</pre>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zpool status zpool</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">  pool: zpool
 state: ONLINE
  scan: scrub repaired 0 in 4h22m with 0 errors on Fri Aug 28 23:52:55 2015
config:

        NAME        STATE     READ WRITE CKSUM
        zpool       ONLINE       0     0     0
          sde       ONLINE       0     0     0
          sdd       ONLINE       0     0     0
          sdf       ONLINE       0     0     0

errors: No known data errors
</pre>
<h2><span class="mw-headline" id="Creating_and_destroying_datasets">Creating and destroying datasets</span></h2>
<p>An example creating child datasets and using compression:
</p>
<ul><li>create the datasets</li></ul>
<pre># zfs create -p -o compression=on san/vault/falcon/snapshots
# zfs create -o compression=on san/vault/falcon/version
# zfs create -p -o compression=on san/vault/redtail/c/Users
</pre>
<ul><li>now list the datasets (this was a linear span)</li></ul>
<pre>$ zfs list
</pre>
<p>Note, there is a huge advantage(file deletion) for making a 3 level dataset.  If you have large amounts of data, by separating by datasets, its easier to destroy a dataset than to try and wait for recursive file removal to complete.
</p>
<h2><span class="mw-headline" id="Displaying_and_setting_properties">Displaying and setting properties</span></h2>
<p>Without specifying them in the creation step, users can set properties of their zpools at any time after its creation using <code>/usr/bin/zfs</code>.
</p>
<h3><span class="mw-headline" id="Show_properties">Show properties</span></h3>
<p>To see the current properties of a given zpool:
</p>
<pre># zfs get all zpool
</pre>
<h3><span class="mw-headline" id="Modify_properties">Modify properties</span></h3>
<p>Disable the recording of access time in the zpool:
</p>
<pre># zfs set atime=off zpool
</pre>
<p>Verify that the property has been set on the zpool:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs get atime</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME  PROPERTY     VALUE     SOURCE
zpool  atime        off       local
</pre>
<div class="archwiki-template-box archwiki-template-box-tip">
<strong>Tip:</strong> This option like many others can be toggled off when creating the zpool as well by appending the following to the creation step: -O atime-off</div>
<h2><span class="mw-headline" id="Add_content_to_the_Zpool_and_query_compression_performance">Add content to the Zpool and query compression performance</span></h2>
<p>Fill the zpool with files.  For this example, first enable compression.  ZFS uses many compression types, including, lzjb, gzip, gzip-N, zle, and lz4.  Using a setting of simply 'on' will call the default algorithm (lzjb) but lz4 is a nice alternative.  See the zfs man page for more.
</p>
<pre># zfs set compression=lz4 zpool
</pre>
<p>In this example, the linux source tarball is copied over and since lz4 compression has been enabled on the zpool, the corresponding compression ratio can be queried as well.
</p>
<pre>$ wget <a rel="nofollow" class="external free" href="https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.11.tar.xz">https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.11.tar.xz</a>
$ tar xJf linux-3.11.tar.xz -C /zpool 
</pre>
<p>To see the compression ratio achieved:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs get compressratio</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME      PROPERTY       VALUE  SOURCE
zpool  compressratio  2.32x  -
</pre>
<h2><span class="mw-headline" id="Simulate_a_disk_failure_and_rebuild_the_Zpool">Simulate a disk failure and rebuild the Zpool</span></h2>
<p>To simulate catastrophic disk failure (i.e. one of the HDDs in the zpool stops functioning), zero out one of the VDEVs.
</p>
<pre>$ dd if=/dev/zero of=/scratch/2.img bs=4M count=1 2&gt;/dev/null
</pre>
<p>Since we used a blocksize (bs) of 4M, the once 2G image file is now a mere 4M:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ ls -lh /scratch </pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">total 317M
-rw-r--r-- 1 facade users 2.0G Oct 20 09:13 1.img
-rw-r--r-- 1 facade users 4.0M Oct 20 09:09 2.img
-rw-r--r-- 1 facade users 2.0G Oct 20 09:13 3.img
</pre>
<p>The zpool remains online despite the corruption.  Note that if a physical disc does fail, <a href="/title/Dmesg" class="mw-redirect" title="Dmesg">dmesg</a> and related logs would be full of errors.  To detect when damage occurs, users must execute a scrub operation.
</p>
<pre># zpool scrub zpool
</pre>
<p>Depending on the size and speed of the underlying media as well as the amount of data in the zpool, the scrub may take hours to complete.
The status of the scrub can be queried:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zpool status zpool</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">  pool: zpool
 state: DEGRADED
status: One or more devices could not be used because the label is missing or
	invalid.  Sufficient replicas exist for the pool to continue
	functioning in a degraded state.
action: Replace the device using 'zpool replace'.
   see: http://zfsonlinux.org/msg/ZFS-8000-4J
  scan: scrub repaired 0 in 0h0m with 0 errors on Sun Oct 20 09:13:39 2013
config:

	NAME                STATE     READ WRITE CKSUM
	   zpool            DEGRADED     0     0     0
	  raidz1-0          DEGRADED     0     0     0
	    /scratch/1.img  ONLINE       0     0     0
	    /scratch/2.img  UNAVAIL      0     0     0  corrupted data
	    /scratch/3.img  ONLINE       0     0     0

errors: No known data errors
</pre>
<p>Since we zeroed out one of our VDEVs, let us simulate adding a new 2G HDD by creating a new image file and adding it to the zpool:
</p>
<pre>$ truncate -s 2G /scratch/new.img
# zpool replace zpool /scratch/2.img /scratch/new.img
</pre>
<p>Upon replacing the VDEV with a new one, zpool rebuilds the data from the data and parity info in the remaining two good VDEVs.  Check the status of this process:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zpool status zpool </pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">  pool: zpool
 state: ONLINE
  scan: resilvered 117M in 0h0m with 0 errors on Sun Oct 20 09:21:22 2013
config:

	NAME                  STATE     READ WRITE CKSUM
	   zpool              ONLINE       0     0     0
	  raidz1-0            ONLINE       0     0     0
	    /scratch/1.img    ONLINE       0     0     0
	    /scratch/new.img  ONLINE       0     0     0
	    /scratch/3.img    ONLINE       0     0     0

errors: No known data errors
</pre>
<h2><span class="mw-headline" id="Snapshots_and_recovering_deleted_files">Snapshots and recovering deleted files</span></h2>
<p>Since ZFS is a copy-on-write filesystem, every file exists the second it is written.  Saving changes to the very same file actually creates another copy of that file (plus the changes made).  Snapshots can take advantage of this fact and allow users access to older versions of files provided a snapshot has been taken.
</p>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> When using snapshots, many Linux programs that report on filesystem space such as <b>df</b> will report inaccurate results due to the unique way snapshots are used on ZFS.  The output of <code>/usr/bin/zfs list</code> will deliver an accurate report of the amount of available and free space on the zpool.</div>
<p>To keep this simple, we will create a dataset within the zpool and snapshot it.  Snapshots can be taken either of the entire zpool or of a dataset within the pool.  They differ only in their naming conventions:
</p>
<table class="wikitable" align="center">

<tbody>
<tr>
<th>Snapshot Target</th>
<th>Snapshot Name
</th>
</tr>
<tr>
<td>Entire zpool</td>
<td>zpool@snapshot-name
</td>
</tr>
<tr>
<td>Dataset</td>
<td>zpool/dataset@snapshot-name
</td>
</tr>
</tbody>
</table>
<p>Make a new data set and take ownership of it.
</p>
<pre># zfs create zpool/docs
# chown facade:users /zpool/docs
</pre>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> The lack of a proceeding / in the create command is intentional, not a typo!</div>
<h3><span class="mw-headline" id="Time_0">Time 0</span></h3>
<p>Add some files to the new dataset (/zpool/docs):
</p>
<pre>$ wget -O /zpool/docs/Moby_Dick.txt  <a rel="nofollow" class="external free" href="https://www.gutenberg.org/ebooks/2701.txt.utf-8">https://www.gutenberg.org/ebooks/2701.txt.utf-8</a>
$ wget -O /zpool/docs/War_and_Peace.txt <a rel="nofollow" class="external free" href="https://www.gutenberg.org/ebooks/2600.txt.utf-8">https://www.gutenberg.org/ebooks/2600.txt.utf-8</a>
$ wget -O /zpool/docs/Beowulf.txt <a rel="nofollow" class="external free" href="https://www.gutenberg.org/ebooks/16328.txt.utf-8">https://www.gutenberg.org/ebooks/16328.txt.utf-8</a>
</pre>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME           USED  AVAIL  REFER  MOUNTPOINT
zpool       5.06M  3.91G  40.0K  /zpool
zpool/docs  4.92M  3.91G  4.92M  /zpool/docs
</pre>
<p>This is showing that we have 4.92M of data used by our books in /zpool/docs.
</p>
<h3>
<span id="Time_.2B1"></span><span class="mw-headline" id="Time_+1">Time +1</span>
</h3>
<p>Now take a snapshot of the dataset:
</p>
<pre># zfs snapshot zpool/docs@001
</pre>
<p>Again run the list command:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME           USED  AVAIL  REFER  MOUNTPOINT
zpool       5.07M  3.91G  40.0K  /zpool
zpool/docs  4.92M  3.91G  4.92M  /zpool/docs
</pre>
<p>Note that the size in the USED col did not change showing that the snapshot take up no space in the zpool since nothing has changed in these three files.
</p>
<p>We can list out the snapshots like so and again confirm the snapshot is taking up no space, but instead <b>refers to</b> files from the originals that take up, 4.92M (their original size):
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list -t snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME               USED  AVAIL  REFER  MOUNTPOINT
zpool/docs@001      0      -  4.92M  -
</pre>
<h3>
<span id="Time_.2B2"></span><span class="mw-headline" id="Time_+2">Time +2</span>
</h3>
<p>Now let us add some additional content and create a new snapshot:
</p>
<pre>$ wget -O /zpool/docs/Les_Mis.txt <a rel="nofollow" class="external free" href="https://www.gutenberg.org/ebooks/135.txt.utf-8">https://www.gutenberg.org/ebooks/135.txt.utf-8</a>
# zfs snapshot zpool/docs@002
</pre>
<p>Generate the new list to see how the space has changed:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list -t snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME               USED  AVAIL  REFER  MOUNTPOINT
zpool/docs@001  25.3K      -  4.92M  -
zpool/docs@002      0      -  8.17M  -
</pre>
<p>Here we can see that the 001 snapshot takes up 25.3K of metadata and still points to the original 4.92M of data, and the new snapshot takes-up no space and refers to a total of 8.17M.
</p>
<h3>
<span id="Time_.2B3"></span><span class="mw-headline" id="Time_+3">Time +3</span>
</h3>
<p>Now let us simulate an accidental overwrite of a file and subsequent data loss:
</p>
<pre>$ echo "this book sucks" &gt; /zpool/docs/War_and_Peace.txt
</pre>
<p>Again, take another snapshot:
</p>
<pre># zfs snapshot zpool/docs@003
</pre>
<p>Now list out the snapshots and notice the amount of referred to decreased by about 3.1M:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list -t snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME               USED  AVAIL  REFER  MOUNTPOINT
zpool/docs@001  25.3K      -  4.92M  -
zpool/docs@002  25.5K      -  8.17M  -
zpool/docs@003      0      -  5.04M  -
</pre>
<p>We can easily recover from this situation by looking inside one or both of our older snapshots for good copy of the file.  ZFS stores its snapshots in a hidden directory under the zpool: <code>/zpool/files/.zfs/snapshot</code>:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ ls -l /zpool/docs/.zfs/snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">total 0
dr-xr-xr-x 1 root root 0 Oct 20 16:09 001
dr-xr-xr-x 1 root root 0 Oct 20 16:09 002
dr-xr-xr-x 1 root root 0 Oct 20 16:09 003
</pre>
<p>We can copy a good version of the book back out from any of our snapshots to any location on or off the zpool:
</p>
<pre>% cp /zpool/docs/.zfs/snapshot/002/War_and_Peace.txt /zpool/docs
</pre>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> Using &lt;TAB&gt; for autocompletion will not work by default but can be changed by modifying the <i>snapdir</i> property on the pool or dataset.</div>
<pre># zfs set snapdir=visible zpool/docs
</pre>
<p>Now enter a snapshot dir or two:
</p>
<pre>$ cd /zpool/docs/.zfs/snapshot/001
$ cd /zpool/docs/.zfs/snapshot/002
</pre>
<p>Repeat the df command:
</p>
<pre>$ df -h | grep zpool
zpool           4.0G     0  4.0G   0% /zpool
zpool/docs      4.0G  5.0M  4.0G   1% /zpool/docs
zpool/docs@001  4.0G  4.9M  4.0G   1% /zpool/docs/.zfs/snapshot/001
zpool/docs@002  4.0G  8.2M  4.0G   1% /zpool/docs/.zfs/snapshot/002
</pre>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> Seeing each dir under .zfs the user enters is reversible if the zpool is taken offline and then remounted or if the server is rebooted.</div>
<p>For example:
</p>
<pre># zpool export zpool
# zpool import -d /scratch/ zpool
$ df -h | grep zpool
zpool         4.0G     0  4.0G   0% /zpool
zpool/docs    4.0G  5.0M  4.0G   1% /zpool/docs
</pre>
<h3>
<span id="Time_.2B4"></span><span class="mw-headline" id="Time_+4">Time +4</span>
</h3>
<p>Now that everything is back to normal, we can create another snapshot of this state:
</p>
<pre># zfs snapshot zpool/docs@004
</pre>
<p>And the list now becomes:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list -t snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME               USED  AVAIL  REFER  MOUNTPOINT
zpool/docs@001  25.3K      -  4.92M  -
zpool/docs@002  25.5K      -  8.17M  -
zpool/docs@003   155K      -  5.04M  -
zpool/docs@004      0      -  8.17M  -
</pre>
<h3><span class="mw-headline" id="Listing_snapshots">Listing snapshots</span></h3>
<p>Note, this simple but important command is missing frequently from other articles on the subject, so its worth mention.
</p>
<p>To list any snapshots on your system, run the following command
</p>
<pre>$ zfs list -t snapshot</pre>
<h3><span class="mw-headline" id="Deleting_snapshots">Deleting snapshots</span></h3>
<p>The limit to the number of snapshots users can save is 2^64. User can delete a snapshot like so:
</p>
<pre># zfs destroy zpool/docs@001
</pre>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;"># zfs list -t snapshot</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NAME               USED  AVAIL  REFER  MOUNTPOINT
zpool/docs@002  3.28M      -  8.17M  -
zpool/docs@003   155K      -  5.04M  -
zpool/docs@004      0      -  8.17M  -
</pre>
<h2><span class="mw-headline" id="Troubleshooting">Troubleshooting</span></h2>
<p>If your system is not configured to load the zfs pool upon boot, or for whatever reason you want to manually remove and add back the pool, or if you have lost your pool completely, a convenient way is to use import/export. 
</p>
<p>If your pool was named &lt;zpool&gt;
</p>
<pre># zpool import -d /scratch zpool
</pre>
<p>If you have any problems accessing your pool at any time, try export and reimport. 
</p>
<pre># zpool export zpool
# zpool import -d /scratch zpool
</pre>
</div>
</div>
<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks">
<a href="/title/Special:Categories" title="Special:Categories">Category</a>: <ul><li><a href="/title/Category:File_systems" title="Category:File systems">File systems</a></li></ul>
</div></div>
	</div>
</div>

<footer id="footer" class="mw-footer" role="contentinfo" style="margin: 0">
	<ul id="footer-info">
		<li>Retrieved from "<a dir="ltr" href="https://wiki.archlinux.org/index.php?title=ZFS/Virtual_disks&amp;oldid=669601">https://wiki.archlinux.org/index.php?title=ZFS/Virtual_disks&amp;oldid=669601</a>"</li>
		<li id="footer-info-lastmod"> This page was last edited on 8 May 2021, at 12:01.</li>
		<li id="footer-info-copyright">Content is available under <a class="external" rel="nofollow" href="http://www.gnu.org/copyleft/fdl.html">GNU Free Documentation License 1.3 or later</a> unless otherwise noted.</li>
	<br>
</ul>
	<ul id="footer-places">
		<li id="footer-places-privacy"><a href="/title/ArchWiki:Privacy_policy" title="ArchWiki:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/title/ArchWiki:About" title="ArchWiki:About">About ArchWiki</a></li>
		<li id="footer-places-disclaimer"><a href="/title/ArchWiki:General_disclaimer" title="ArchWiki:General disclaimer">Disclaimers</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico">
	</ul>
	<div style="clear: both;"></div>
</footer>



</body>
</html>
