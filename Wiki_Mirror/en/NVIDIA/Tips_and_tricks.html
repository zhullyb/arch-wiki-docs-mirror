<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>NVIDIA/Tips and tricks - ArchWiki</title>
<link rel="stylesheet" href="../../ArchWikiOffline.css">
<meta name="ResourceLoaderDynamicStyles" content="">
<meta name="generator" content="MediaWiki 1.35.0">
<meta name="referrer" content="no-referrer-when-downgrade">
<meta name="robots" content="noindex,follow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/opensearch_desc.php" title="ArchWiki (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://wiki.archlinux.org/api.php?action=rsd">
<link rel="license" href="http://www.gnu.org/copyleft/fdl.html">
<link rel="alternate" type="application/atom+xml" title="ArchWiki Atom feed" href="/index.php?title=Special:RecentChanges&amp;feed=atom">
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-NVIDIA_Tips_and_tricks rootpage-NVIDIA skin-vector action-view skin-vector-legacy">
<div id="content" class="mw-body" role="main" style="margin: 0">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"></div>
	<div class="mw-indicators mw-body-content">
	</div>
	<h1 id="firstHeading" class="firstHeading" lang="en">NVIDIA/Tips and tricks</h1>
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From ArchWiki</div>
		<div id="contentSub"><span class="subpages">&lt; <a href="/title/NVIDIA" title="NVIDIA">NVIDIA</a></span></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr">
<div class="warningbox">The printable version is no longer supported and may have rendering errors. Please update your browser bookmarks and please use the default browser print function instead.</div>
<div class="mw-parser-output">
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading">
<input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none"><div class="toctitle" lang="en" dir="ltr">
<h2 id="mw-toc-heading">Contents</h2>
<span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Fixing_terminal_resolution"><span class="tocnumber">1</span> <span class="toctext">Fixing terminal resolution</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Using_TV-out"><span class="tocnumber">2</span> <span class="toctext">Using TV-out</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#X_with_a_TV_(DFP)_as_the_only_display"><span class="tocnumber">3</span> <span class="toctext">X with a TV (DFP) as the only display</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Headless_(no_monitor)_resolution"><span class="tocnumber">4</span> <span class="toctext">Headless (no monitor) resolution</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Check_the_power_source"><span class="tocnumber">5</span> <span class="toctext">Check the power source</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Listening_to_ACPI_events"><span class="tocnumber">6</span> <span class="toctext">Listening to ACPI events</span></a></li>
<li class="toclevel-1 tocsection-7">
<a href="#Displaying_GPU_temperature_in_the_shell"><span class="tocnumber">7</span> <span class="toctext">Displaying GPU temperature in the shell</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#nvidia-settings"><span class="tocnumber">7.1</span> <span class="toctext">nvidia-settings</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#nvidia-smi"><span class="tocnumber">7.2</span> <span class="toctext">nvidia-smi</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#nvclock"><span class="tocnumber">7.3</span> <span class="toctext">nvclock</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11">
<a href="#Overclocking_and_cooling"><span class="tocnumber">8</span> <span class="toctext">Overclocking and cooling</span></a>
<ul>
<li class="toclevel-2 tocsection-12">
<a href="#Enabling_overclocking"><span class="tocnumber">8.1</span> <span class="toctext">Enabling overclocking</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Setting_static_2D/3D_clocks"><span class="tocnumber">8.1.1</span> <span class="toctext">Setting static 2D/3D clocks</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Allow_change_to_highest_performance_mode"><span class="tocnumber">8.1.2</span> <span class="toctext">Allow change to highest performance mode</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Saving_overclocking_settings"><span class="tocnumber">8.1.3</span> <span class="toctext">Saving overclocking settings</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-16"><a href="#Custom_TDP_Limit"><span class="tocnumber">8.2</span> <span class="toctext">Custom TDP Limit</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Set_fan_speed_at_login"><span class="tocnumber">8.3</span> <span class="toctext">Set fan speed at login</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-18"><a href="#Kernel_module_parameters"><span class="tocnumber">9</span> <span class="toctext">Kernel module parameters</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#Preserve_video_memory_after_suspend"><span class="tocnumber">10</span> <span class="toctext">Preserve video memory after suspend</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#Driver_persistence"><span class="tocnumber">11</span> <span class="toctext">Driver persistence</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Fixing_terminal_resolution">Fixing terminal resolution</span></h2>
<p>Transitioning from nouveau may cause your startup terminal to display at a lower resolution.
</p>
<p>For GRUB, see <a href="/title/GRUB/Tips_and_tricks#Setting_the_framebuffer_resolution" title="GRUB/Tips and tricks">GRUB/Tips and tricks#Setting the framebuffer resolution</a> for details.
</p>
<p>For <a href="/title/REFInd" title="REFInd">rEFInd</a>, add to <code><i>esp</i>/EFI/refind/refind.conf</code> and <code>/etc/refind.d/refind.conf</code> (latter file is optional but recommended):
</p>
<pre>use_graphics_for linux
</pre>
<p>A small caveat is that this will hide the kernel parameters from being shown during boot.
</p>
<h2><span class="mw-headline" id="Using_TV-out">Using TV-out</span></h2>
<p>A good article on the subject can be found <a rel="nofollow" class="external text" href="https://en.wikibooks.org/wiki/NVidia/TV-OUT">here</a>.
</p>
<h2>
<span id="X_with_a_TV_.28DFP.29_as_the_only_display"></span><span class="mw-headline" id="X_with_a_TV_(DFP)_as_the_only_display">X with a TV (DFP) as the only display</span>
</h2>
<p>The X server falls back to CRT-0 if no monitor is automatically detected. This can be a problem when using a DVI connected TV as the main display, and X is started while the TV is turned off or otherwise disconnected.
</p>
<p>To force NVIDIA to use DFP, store a copy of the EDID somewhere in the filesystem so that X can parse the file instead of reading EDID from the TV/DFP.
</p>
<p>To acquire the EDID, start nvidia-settings. It will show some information in tree format, ignore the rest of the settings for now and select the GPU (the corresponding entry should be titled "GPU-0" or similar), click the <code>DFP</code> section (again, <code>DFP-0</code> or similar), click on the <code>Acquire Edid</code> Button and store it somewhere, for example, <code>/etc/X11/dfp0.edid</code>.
</p>
<p>If in the front-end mouse and keyboard are not attached, the EDID can be acquired using only the command line. Run an X server with enough verbosity to print out the EDID block:
</p>
<pre>$ startx -- -logverbose 6
</pre>
<p>After the X Server has finished initializing, close it and your log file will probably be in <code>/var/log/Xorg.0.log</code>. Extract the EDID block using nvidia-xconfig:
</p>
<pre>$ nvidia-xconfig --extract-edids-from-file=/var/log/Xorg.0.log --extract-edids-output-file=/etc/X11/dfp0.bin
</pre>
<p>Edit <code>xorg.conf</code> by adding to the <code>Device</code> section:
</p>
<pre>Option "ConnectedMonitor" "DFP"
Option "CustomEDID" "DFP-0:/etc/X11/dfp0.edid"
</pre>
<p>The <code>ConnectedMonitor</code> option forces the driver to recognize the DFP as if it were connected. The <code>CustomEDID</code> provides EDID data for the device, meaning that it will start up just as if the TV/DFP was connected during X the process.
</p>
<p>This way, one can automatically start a display manager at boot time and still have a working and properly configured X screen by the time the TV gets powered on.
</p>
<p>If the above changes did not work, in the <code>xorg.conf</code> under <code>Device</code> section you can try to remove the <code>Option "ConnectedMonitor" "DFP"</code> and add the following lines:
</p>
<pre>Option "ModeValidation" "NoDFPNativeResolutionCheck"
Option "ConnectedMonitor" "DFP-0"
</pre>
<p>The <code>NoDFPNativeResolutionCheck</code> prevents NVIDIA driver from disabling all the modes that do not fit in the native resolution.
</p>
<h2>
<span id="Headless_.28no_monitor.29_resolution"></span><span class="mw-headline" id="Headless_(no_monitor)_resolution">Headless (no monitor) resolution</span>
</h2>
<p>In headless mode, resolution falls back to 640x480, which is used by VNC or Steam Link. To start in a higher resolution <i>e.g.</i> 1920x1080, specify a <code>Virtual</code> entry under the <code>Screen</code> subsection in <code>xorg.conf</code>:
</p>
<pre>Section "Screen"
   [...]
   SubSection     "Display"
       Depth       24
       Virtual     1920 1080
   EndSubSection
EndSection
</pre>
<h2><span class="mw-headline" id="Check_the_power_source">Check the power source</span></h2>
<p>The NVIDIA X.org driver can also be used to detect the GPU's current source of power. To see the current power source, check the 'GPUPowerSource' read-only parameter (0 - AC, 1 - battery):
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-settings -q GPUPowerSource -t</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">1</pre>
<h2><span class="mw-headline" id="Listening_to_ACPI_events">Listening to ACPI events</span></h2>
<p>NVIDIA drivers automatically try to connect to the <a href="/title/Acpid" title="Acpid">acpid</a> daemon and listen to ACPI events such as battery power, docking, some hotkeys, etc. If connection fails, X.org will output the following warning:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">~/.local/share/xorg/Xorg.0.log</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">NVIDIA(0): ACPI: failed to connect to the ACPI event daemon; the daemon
NVIDIA(0):     may not be running or the "AcpidSocketPath" X
NVIDIA(0):     configuration option may not be set correctly.  When the
NVIDIA(0):     ACPI event daemon is available, the NVIDIA X driver will
NVIDIA(0):     try to use it to receive ACPI event notifications.  For
NVIDIA(0):     details, please see the "ConnectToAcpid" and
NVIDIA(0):     "AcpidSocketPath" X configuration options in Appendix B: X
NVIDIA(0):     Config Options in the README.
</pre>
<p>While completely harmless, you may get rid of this message by disabling the <code>ConnectToAcpid</code> option in your <code>/etc/X11/xorg.conf.d/20-nvidia.conf</code>:
</p>
<pre>Section "Device"
  ...
  Driver "nvidia"
  Option "ConnectToAcpid" "0"
  ...
EndSection
</pre>
<p>If you are on laptop, it might be a good idea to install and enable the <a href="/title/Acpid" title="Acpid">acpid</a> daemon instead.
</p>
<h2><span class="mw-headline" id="Displaying_GPU_temperature_in_the_shell">Displaying GPU temperature in the shell</span></h2>
<p>There are three methods to query the GPU temperature. <i>nvidia-settings</i> requires that you are using X, <i>nvidia-smi</i> or <i>nvclock</i> do not. Also note that <i>nvclock</i> currently does not work with newer NVIDIA cards such as GeForce 200 series cards as well as embedded GPUs such as the Zotac IONITX's 8800GS.
</p>
<h3><span class="mw-headline" id="nvidia-settings">nvidia-settings</span></h3>
<p>To display the GPU temp in the shell, use <i>nvidia-settings</i> as follows:
</p>
<pre>$ nvidia-settings -q gpucoretemp
</pre>
<p>This will output something similar to the following:
</p>
<pre>Attribute 'GPUCoreTemp' (hostname:0.0): 41.
'GPUCoreTemp' is an integer attribute.
'GPUCoreTemp' is a read-only attribute.
'GPUCoreTemp' can use the following target types: X Screen, GPU.
</pre>
<p>The GPU temps of this board is 41 C.
</p>
<p>In order to get just the temperature for use in utilities such as <i>rrdtool</i> or <i>conky</i>:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-settings -q gpucoretemp -t</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">41</pre>
<h3><span class="mw-headline" id="nvidia-smi">nvidia-smi</span></h3>
<p>Use <i>nvidia-smi</i> which can read temps directly from the GPU without the need to use X at all, e.g. when running Wayland or on a headless server. 
To display the GPU temperature in the shell, use <i>nvidia-smi</i> as follows:
</p>
<pre>$ nvidia-smi
</pre>
<p>This should output something similar to the following:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">Fri Jan  6 18:53:54 2012       
+------------------------------------------------------+                       
| NVIDIA-SMI 2.290.10   Driver Version: 290.10         |                       
|-------------------------------+----------------------+----------------------+
| Nb.  Name                     | Bus Id        Disp.  | Volatile ECC SB / DB |
| Fan   Temp   Power Usage /Cap | Memory Usage         | GPU Util. Compute M. |
|===============================+======================+======================|
| 0.  GeForce 8500 GT           | 0000:01:00.0  N/A    |       N/A        N/A |
|  30%   62 C  N/A   N/A /  N/A |  17%   42MB /  255MB |  N/A      Default    |
|-------------------------------+----------------------+----------------------|
| Compute processes:                                               GPU Memory |
|  GPU  PID     Process name                                       Usage      |
|=============================================================================|
|  0.           ERROR: Not Supported                                          |
+-----------------------------------------------------------------------------+
</pre>
<p>Only for temperature:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi -q -d TEMPERATURE</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">

====NVSMI LOG====

Timestamp                           : Sun Apr 12 08:49:10 2015
Driver Version                      : 346.59

Attached GPUs                       : 1
GPU 0000:01:00.0
    Temperature
        GPU Current Temp            : 52 C
        GPU Shutdown Temp           : N/A
        GPU Slowdown Temp           : N/A

</pre>
<p>In order to get just the temperature for use in utilities such as <i>rrdtool</i> or <i>conky</i>:
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">$ nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">52</pre>
<p>Reference: <a rel="nofollow" class="external free" href="https://www.question-defense.com/2010/03/22/gpu-linux-shell-temp-get-nvidia-gpu-temperatures-via-linux-cli">https://www.question-defense.com/2010/03/22/gpu-linux-shell-temp-get-nvidia-gpu-temperatures-via-linux-cli</a>.
</p>
<h3><span class="mw-headline" id="nvclock">nvclock</span></h3>
<p>Use <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvclock/">nvclock</a></span><sup><small>AUR</small></sup> which is available from the <a href="/title/AUR" class="mw-redirect" title="AUR">AUR</a>.
</p>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> <i>nvclock</i> cannot access thermal sensors on newer NVIDIA cards such as Geforce 200 series cards.</div>
<p>There can be significant differences between the temperatures reported by <i>nvclock</i> and <i>nvidia-settings</i>/<i>nv-control</i>. According to <a rel="nofollow" class="external text" href="https://sourceforge.net/projects/nvclock/forums/forum/67426/topic/1906899">this post</a> by the author (thunderbird) of <i>nvclock</i>, the <i>nvclock</i> values should be more accurate.
</p>
<h2><span class="mw-headline" id="Overclocking_and_cooling">Overclocking and cooling</span></h2>
<h3><span class="mw-headline" id="Enabling_overclocking">Enabling overclocking</span></h3>
<div class="archwiki-template-box archwiki-template-box-warning">
<strong>Warning:</strong> Overclocking might permanently damage your hardware. You have been warned.</div>
<p>Overclocking is controlled via <i>Coolbits</i> option in the <code>Device</code> section, which enables various unsupported features:
</p>
<pre>Option "Coolbits" "<i>value</i>"
</pre>
<div class="archwiki-template-box archwiki-template-box-tip">
<strong>Tip:</strong> The <i>Coolbits</i> option can be easily controlled with the <i>nvidia-xconfig</i>, which manipulates the Xorg configuration files: <pre># nvidia-xconfig --cool-bits=<i>value</i></pre>
</div>
<p>The <i>Coolbits</i> value is the sum of its component bits in the binary numeral system. The component bits are:
</p>
<ul>
<li>
<code>1</code> (bit 0) - Enables overclocking of older (pre-Fermi) cores on the <i>Clock Frequencies</i> page in <i>nvidia-settings</i>.</li>
<li>
<code>2</code> (bit 1) - When this bit is set, the driver will "attempt to initialize SLI when using GPUs with different amounts of video memory".</li>
<li>
<code>4</code> (bit 2) - Enables manual configuration of GPU fan speed on the <i>Thermal Monitor</i> page in <i>nvidia-settings</i>.</li>
<li>
<code>8</code> (bit 3) - Enables overclocking on the <i>PowerMizer</i> page in <i>nvidia-settings</i>. Available since version 337.12 for the Fermi architecture and newer.<a rel="nofollow" class="external autonumber" href="https://www.phoronix.com/scan.php?px=MTY1OTM&amp;page=news_item">[1]</a>
</li>
<li>
<code>16</code> (bit 4) - Enables overvoltage using <i>nvidia-settings</i> CLI options. Available since version 346.16 for the Fermi architecture and newer.<a rel="nofollow" class="external autonumber" href="https://www.phoronix.com/scan.php?page=news_item&amp;px=MTg0MDI">[2]</a>
</li>
</ul>
<p>To enable multiple features, add the <i>Coolbits</i> values together. For example, to enable overclocking and overvoltage of Fermi cores, set <code>Option "Coolbits" "24"</code>.
</p>
<p>The documentation of <i>Coolbits</i> can be found in <code>/usr/share/doc/nvidia/html/xconfigoptions.html</code> and <a rel="nofollow" class="external text" href="https://download.nvidia.com/XFree86/Linux-x86_64/430.14/README/xconfigoptions.html#Coolbits">here</a>.
</p>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> An alternative is to edit and reflash the GPU BIOS either under DOS (preferred), or within a Win32 environment by way of <a rel="nofollow" class="external text" href="https://www.techpowerup.com/download/nvidia-nvflash/">nvflash</a> and <a rel="nofollow" class="external text" href="https://www.guru3d.com/files-details/nvidia-bios-editor-download-nibitor.html">NiBiTor 6.0</a>. The advantage of BIOS flashing is that not only can voltage limits be raised, but stability is generally improved over software overclocking methods such as Coolbits. <a rel="nofollow" class="external text" href="https://ivanvojtko.blogspot.sk/2014/03/how-to-overclock-geforce-460gtx-fermi.html">Fermi BIOS modification tutorial</a>
</div>
<h4>
<span id="Setting_static_2D.2F3D_clocks"></span><span class="mw-headline" id="Setting_static_2D/3D_clocks">Setting static 2D/3D clocks</span>
</h4>
<p>Set the following string in the <code>Device</code> section to enable PowerMizer at its maximum performance level (VSync will not work without this line):
</p>
<pre>Option "RegistryDwords" "PerfLevelSrc=0x2222"
</pre>
<h4><span class="mw-headline" id="Allow_change_to_highest_performance_mode">Allow change to highest performance mode</span></h4>
<div class="noprint archwiki-template-message">
<p><a href="/title/File:Tango-inaccurate.png" class="image"><img alt="Tango-inaccurate.png" src="../../File:Tango-inaccurate.png" decoding="async" width="48" height="48"></a><b>The factual accuracy of this article or section is disputed.</b><a href="/title/File:Tango-inaccurate.png" class="image"><img alt="Tango-inaccurate.png" src="../../File:Tango-inaccurate.png" decoding="async" width="48" height="48"></a></p>
<div>
<b>Reason:</b> This section refers to the limits for <a href="https://en.wikipedia.org/wiki/Kepler_(microarchitecture)#GPU_Boost" class="extiw" title="wikipedia:Kepler (microarchitecture)">GPU boost</a>, which is unrelated to overclocking discussed above. The <code>nvidia-smi(1)</code> man page says that it is "For Tesla devices from the Kepler+ family and Maxwell-based GeForce Titan." And as far as <a href="/title/User:Lahwaacz" title="User:Lahwaacz">Lahwaacz</a> is aware, the only GPU which supports this and does not have the default clocks equal to the maximum, is Tesla K40 <a rel="nofollow" class="external autonumber" href="https://www.nvidia.com/content/PDF/kepler/nvidia-gpu-boost-tesla-k40-06767-001-v02.pdf">[3]</a>. Since the Pascal architecture, <a rel="nofollow" class="external text" href="https://www.anandtech.com/show/10325/the-nvidia-geforce-gtx-1080-and-1070-founders-edition-review/15">Boost 3.0</a> handles automatic clocking even differently. (Discuss in <a rel="nofollow" class="external text" href="https://wiki.archlinux.org/title/Talk:NVIDIA/Tips_and_tricks">Talk:NVIDIA/Tips and tricks#</a>)</div>
</div>
<p>Since changing performance mode and overclocking memory rate has little to no effect in <i>nvidia-settings</i>, try this:
</p>
<ul>
<li>Setting Coolbits to 24 or 28 and remove Powermizer RegistryDwords -&gt; Restart X</li>
<li>find out max. Clock and Memory rate. (this can be LOWER than what your gfx card reports after booting!): <pre>$ nvidia-smi -q -d SUPPORTED_CLOCKS</pre>
</li>
<li>set rates for GPU 0: <pre># nvidia-smi -i 0 -ac memratemax,clockratemax</pre>
</li>
</ul>
<p>After setting the rates the max. performance mode works in <i>nvidia-settings</i> and you can overclock graphics-clock and memory transfer rate.
</p>
<h4><span class="mw-headline" id="Saving_overclocking_settings">Saving overclocking settings</span></h4>
<p>Typically, clock and voltage offsets inserted in the <i>nvidia-settings</i> interface are not saved, being lost after a reboot.
Fortunately, there are tools that offer an interface for overclocking under the proprietary driver, able to save the user's overclocking
preferences and automatically applying them on boot. 
Some of them are:
</p>
<ul>
<li>
<span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/gwe/">gwe</a></span><sup><small>AUR</small></sup> - graphical, applies settings on desktop session start</li>
<li>
<span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvclock/">nvclock</a></span><sup><small>AUR</small></sup> and <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/systemd-nvclock-unit/">systemd-nvclock-unit</a></span><sup><small>AUR</small></sup> - graphical, applies settings on system boot</li>
<li>
<span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://aur.archlinux.org/packages/nvoc/">nvoc</a></span><sup><small>AUR</small></sup> - text based, profiles are configuration files in <code>/etc/nvoc.d/</code>, applies settings on desktop session start</li>
</ul>
<h3><span class="mw-headline" id="Custom_TDP_Limit">Custom TDP Limit</span></h3>
<p>Modern Nvidia graphics cards throttle frequency to stay in their TDP and temperature limits. To increase performance it is possible to change the TDP limit, which will result in higher temperatures and higher power consumption. 
</p>
<p>For example, to set the power limit to 160.30W:
</p>
<pre># nvidia-smi -pl 160.30
</pre>
<p>To set the power limit on boot (without driver persistence):
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/systemd/system/nvidia-tdp.timer</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">[Unit]
Description=Set NVIDIA power limit on boot

[Timer]
OnBootSec=5

[Install]
WantedBy=timers.target</pre>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/systemd/system/nvidia-tdp.service</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">[Unit]
Description=Set NVIDIA power limit

[Service]
Type=oneshot
ExecStart=/usr/bin/nvidia-smi -pl 160.30</pre>
<h3><span class="mw-headline" id="Set_fan_speed_at_login">Set fan speed at login</span></h3>
<div class="noprint archwiki-template-message">
<p><a href="/title/File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" decoding="async" width="48" height="48"></a><b>This article or section needs language, wiki syntax or style improvements. See <a href="/title/Help:Style" title="Help:Style">Help:Style</a> for reference.</b><a href="/title/File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" decoding="async" width="48" height="48"></a></p>
<div>
<b>Reason:</b> Refer to <a href="#Enabling_overclocking">#Enabling overclocking</a> for description of <i>Coolbits</i>. (Discuss in <a rel="nofollow" class="external text" href="https://wiki.archlinux.org/title/Talk:NVIDIA/Tips_and_tricks">Talk:NVIDIA/Tips and tricks#</a>)</div>
</div>
<p>You can adjust the fan speed on your graphics card with <i>nvidia-settings'</i> console interface. First ensure that your Xorg configuration has enabled the bit 2 in the <a href="#Enabling_overclocking">Coolbits</a> option.
</p>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> GeForce 400/500 series cards cannot currently set fan speeds at login using this method. This method only allows for the setting of fan speeds within the current X session by way of <i>nvidia-settings</i>.</div>
<p>Place the following line in your <a href="/title/Xinitrc" class="mw-redirect" title="Xinitrc">xinitrc</a> file to adjust the fan when you launch Xorg. Replace <code><i>n</i></code> with the fan speed percentage you want to set.
</p>
<pre>nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUTargetFanSpeed=<i>n</i>"
</pre>
<p>You can also configure a second GPU by incrementing the GPU and fan number.
</p>
<pre>nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUTargetFanSpeed=<i>n</i>" \
                -a "[gpu:1]/GPUFanControlState=1" -a  [fan:1]/GPUTargetFanSpeed=<i>n</i>" &amp;
</pre>
<p>If you use a login manager such as <a href="/title/GDM" title="GDM">GDM</a> or <a href="/title/SDDM" title="SDDM">SDDM</a>, you can create a desktop entry file to process this setting. Create <code>~/.config/autostart/nvidia-fan-speed.desktop</code> and place this text inside it. Again, change <code><i>n</i></code> to the speed percentage you want.
</p>
<pre>[Desktop Entry]
Type=Application
Exec=nvidia-settings -a "[gpu:0]/GPUFanControlState=1" -a "[fan:0]/GPUTargetFanSpeed=<i>n</i>"
X-GNOME-Autostart-enabled=true
Name=nvidia-fan-speed
</pre>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> Before driver version 349.16, <code>GPUCurrentFanSpeed</code> was used instead of <code>GPUTargetFanSpeed</code>.<a rel="nofollow" class="external autonumber" href="https://devtalk.nvidia.com/default/topic/821563/linux/can-t-control-fan-speed-with-beta-driver-349-12/post/4526208/#4526208">[4]</a>
</div>
<p>To make it possible to adjust the fanspeed of more than one graphics card, run:
</p>
<pre>$ nvidia-xconfig --enable-all-gpus
$ nvidia-xconfig --cool-bits=4
</pre>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> On some laptops (including the ThinkPad <a rel="nofollow" class="external text" href="https://devtalk.nvidia.com/default/topic/1052110/linux/can-t-control-gtx-1050-ti-max-q-fan-on-thinkpad-x1-extreme-laptop/post/5340658/#5340658">X1 Extreme</a> and <a rel="nofollow" class="external text" href="https://devtalk.nvidia.com/default/topic/1048624/linux/how-to-set-gpu-fan-speed/post/5321818/#5321818">P51/P52</a>), there are two fans, but neither are controlled by nvidia.</div>
<h2><span class="mw-headline" id="Kernel_module_parameters">Kernel module parameters</span></h2>
<div class="noprint archwiki-template-message">
<p><a href="/title/File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" decoding="async" width="48" height="48"></a><b>This article or section needs language, wiki syntax or style improvements. See <a href="/title/Help:Style" title="Help:Style">Help:Style</a> for reference.</b><a href="/title/File:Tango-edit-clear.png" class="image"><img alt="Tango-edit-clear.png" src="../../File:Tango-edit-clear.png" decoding="async" width="48" height="48"></a></p>
<div>
<b>Reason:</b> Giving advanced examples without explaining what they do is pointless. (Discuss in <a rel="nofollow" class="external text" href="https://wiki.archlinux.org/title/Talk:NVIDIA/Tips_and_tricks">Talk:NVIDIA/Tips and tricks#</a>)</div>
</div>
<p>Some options can be set as kernel module parameters, a full list can be obtained by running <code>modinfo nvidia</code> or looking at <code>nv-reg.h</code>. See <a rel="nofollow" class="external text" href="https://wiki.gentoo.org/wiki/NVidia/nvidia-drivers#Kernel_module_parameters">the Gentoo wiki</a> as well.
</p>
<p>For example, enabling the following will turn on kernel mode setting (see above) and enable the PAT feature <a rel="nofollow" class="external autonumber" href="https://www.kernel.org/doc/html/latest/x86/pat.html">[5]</a>, which affects how memory is allocated. PAT was first introduced in Pentium III <a rel="nofollow" class="external autonumber" href="https://www.kernel.org/doc/ols/2008/ols2008v2-pages-135-144.pdf">[6]</a> and is supported by most newer CPUs (see <a href="https://en.wikipedia.org/wiki/Page_attribute_table#Processors" class="extiw" title="wikipedia:Page attribute table">wikipedia:Page attribute table#Processors</a>). If your system can support this feature, it should improve performance.
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/modprobe.d/nvidia.conf</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">options nvidia-drm modeset=1 
options nvidia NVreg_UsePageAttributeTable=1</pre>
<p>On some notebooks, to enable any nvidia settings tweaking you must include this option, otherwise it responds with "Setting applications clocks is not supported" etc.
</p>
<pre style="margin-bottom: 0; border-bottom:none; padding-bottom:0.8em;">/etc/modprobe.d/nvidia.conf</pre>
<pre style="margin-top: 0; border-top-style:dashed; padding-top: 0.8em;">options nvidia NVreg_RegistryDwords="OverrideMaxPerf=0x1"</pre>
<h2><span class="mw-headline" id="Preserve_video_memory_after_suspend">Preserve video memory after suspend</span></h2>
<p>By default the NVIDIA Linux drivers save and restore only essential video memory allocations on system suspend and resume. Quoting NVIDIA (<a rel="nofollow" class="external autonumber" href="https://download.nvidia.com/XFree86/Linux-x86_64/460.56/README/powermanagement.html">[7]</a>, also available with the <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://archlinux.org/packages/?name=nvidia-utils">nvidia-utils</a></span> package in <b>/usr/share/doc/nvidia/html/powermanagement.html</b>): <i>The resulting loss of video memory contents is partially compensated for by the user-space NVIDIA drivers, and by some applications, but can lead to failures such as rendering corruption and application crashes upon exit from power management cycles.</i>
</p>
<p>The <b>still experimental</b> system enables saving all video memory (given enough space on disk or main RAM). The interface is through the <b>/proc/driver/nvidia/suspend</b> file as follows: write "suspend" (or "hibernate") to <b>/proc/driver/nvidia/suspend</b> immediately before writing to the usual Linux <b>/sys/power/state</b> file, write "resume" to <b>/proc/driver/nvidia/suspend</b> immediately after waking up, or after an unsuccessful attempt to suspend or hibernate.
</p>
<p>The NVIDIA drivers rely on a user defined file system for storage. The chosen file system needs to support unnamed temporary files (ext4 works) and have sufficient capacity for storing the video memory allocations (e.g., at least <code>(sum of the memory capacities of all NVIDIA GPUs) * 1.2</code>). Use the command <code>nvidia-smi -q -d MEMORY</code> to list the memory capacities of all GPUs in the system.
</p>
<p>To choose the file system used for storing video memory during system sleep (and change the default video memory save/restore strategy to save and restore all video memory allocations), it is necessary to pass two options to the "nvidia" kernel module. For example, write the following line to <b>/etc/modprobe.d/nvidia-power-management.conf</b> and reboot:
</p>
<pre>options nvidia NVreg_PreserveVideoMemoryAllocations=1 NVreg_TemporaryFilePath=/tmp-nvidia
</pre>
<p>Feel free to replace "/tmp-nvidia" in the previous line with a path within your desired file system.
</p>
<p>The interaction with <b>/proc/driver/nvidia/suspend</b> is handled by the simple Unix shell script at <b>/usr/bin/nvidia-sleep.sh</b>, which will itself be called by a tool like <a href="/title/Systemd" title="Systemd">Systemd</a>. The Archlinux <span class="plainlinks archwiki-template-pkg"><a rel="nofollow" class="external text" href="https://archlinux.org/packages/?name=nvidia-utils">nvidia-utils</a></span> package ships with the following relevant Systemd services (which essentially just call <b>nvidia-sleep.sh</b>): <code>nvidia-suspend</code>, <code>nvidia-hibernate</code>, <code>nvidia-resume</code>. Contrary to NVIDIA's instructions, it is currently not necessary to enable <code>nvidia-resume</code> (and it is in fact probably not a good idea to enable it), because the <b>/usr/lib/systemd/system-sleep/nvidia</b> script does the same thing as the service (but slightly earlier), and it is enabled by default (Systemd calls it after waking up from a suspend). Do enable <code>nvidia-suspend</code> and/or <code>nvidia-hibernate</code>.
</p>
<h2><span class="mw-headline" id="Driver_persistence">Driver persistence</span></h2>
<p>Nvidia has a daemon that can be optionally run at boot. In a standard single-GPU X desktop environment the persistence daemon is not needed and can actually create issues <a rel="nofollow" class="external autonumber" href="https://devtalk.nvidia.com/default/topic/1044421/linux/nvidia-persistenced-causing-60-second-reboot-delays">[8]</a>. See the <a rel="nofollow" class="external text" href="https://docs.nvidia.com/deploy/driver-persistence/index.html#persistence-daemon">Driver Persistence</a> section of the Nvidia documentation for more details.
</p>
<p>To start the persistence daemon at boot, <a href="/title/Enable" class="mw-redirect" title="Enable">enable</a> the <code>nvidia-persistenced.service</code>. For manual usage see the <a rel="nofollow" class="external text" href="https://docs.nvidia.com/deploy/driver-persistence/index.html#usage">upstream documentation</a>.
</p>
</div>
</div>
<div id="catlinks" class="catlinks" data-mw="interface">
<div id="mw-normal-catlinks" class="mw-normal-catlinks">
<a href="/title/Special:Categories" title="Special:Categories">Categories</a>: <ul>
<li><a href="/title/Category:Graphics" title="Category:Graphics">Graphics</a></li>
<li><a href="/title/Category:X_server" title="Category:X server">X server</a></li>
</ul>
</div>
<div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul>
<li><a href="/title/Category:Pages_or_sections_flagged_with_Template:Accuracy" title="Category:Pages or sections flagged with Template:Accuracy">Pages or sections flagged with Template:Accuracy</a></li>
<li><a href="/title/Category:Pages_or_sections_flagged_with_Template:Style" title="Category:Pages or sections flagged with Template:Style">Pages or sections flagged with Template:Style</a></li>
</ul>
</div>
</div>
	</div>
</div>

<footer id="footer" class="mw-footer" role="contentinfo" style="margin: 0">
	<ul id="footer-info">
		<li>Retrieved from "<a dir="ltr" href="https://wiki.archlinux.org/index.php?title=NVIDIA/Tips_and_tricks&amp;oldid=670644">https://wiki.archlinux.org/index.php?title=NVIDIA/Tips_and_tricks&amp;oldid=670644</a>"</li>
		<li id="footer-info-lastmod"> This page was last edited on 12 May 2021, at 07:39.</li>
		<li id="footer-info-copyright">Content is available under <a class="external" rel="nofollow" href="http://www.gnu.org/copyleft/fdl.html">GNU Free Documentation License 1.3 or later</a> unless otherwise noted.</li>
	<br>
</ul>
	<ul id="footer-places">
		<li id="footer-places-privacy"><a href="/title/ArchWiki:Privacy_policy" title="ArchWiki:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/title/ArchWiki:About" title="ArchWiki:About">About ArchWiki</a></li>
		<li id="footer-places-disclaimer"><a href="/title/ArchWiki:General_disclaimer" title="ArchWiki:General disclaimer">Disclaimers</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico">
	</ul>
	<div style="clear: both;"></div>
</footer>



</body>
</html>
